services:
  rabbitmq:
    image: rabbitmq:3-management
    ports:
      - "5672:5672"
      - "15672:15672"
    environment:
      RABBITMQ_DEFAULT_USER: guest
      RABBITMQ_DEFAULT_PASS: guest

  oraculo:
    build: ./Oraculo
    depends_on:
      cicflowmeter:
        condition: service_healthy
      elasticsearch:
        condition: service_healthy
      logstash:
        condition: service_healthy
    ports:
      - "8000:8000"
    mem_limit: 8g

  mlflow:
    build: ./mlflow
    container_name: mlflow
    ports:
      - "5000:5000"
    environment:
      # Where to keep runs/artifacts in the container
      # (bind-mount/volume below makes them persistent)
      MLFLOW_BACKEND_URI: sqlite:////mlflow/mlflow.db
      MLFLOW_ARTIFACT_ROOT: /mlflow/artifacts
    volumes:
      - mlflow_data:/mlflow
    command: >
      mlflow server
      --host 0.0.0.0
      --port 5000
      --backend-store-uri ${MLFLOW_BACKEND_URI}
      --artifacts-destination ${MLFLOW_ARTIFACT_ROOT}

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:9.0.4
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - xpack.license.self_generated.type=basic
      - ES_JAVA_OPTS=-Xms1g -Xmx1g
    ports:
      - "9200:9200"
    volumes:
      - esdata_new_comparative:/usr/share/elasticsearch/data
    profiles:
      - dev
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:9200"]
      interval: 10s
      timeout: 3s
      retries: 30

  logstash:
    image: docker.elastic.co/logstash/logstash:9.0.4
    container_name: logstash
    depends_on:
      - elasticsearch
    volumes:
      - ./logstash/pipeline:/usr/share/logstash/pipeline
    ports:
      - "5044:5044"
      - "9600:9600"
    environment:
      - LS_JAVA_OPTS=-Xms512m -Xmx512m
    profiles:
      - dev
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:9600/_node/pipelines"]
      interval: 10s
      timeout: 3s
      retries: 30
    

  kibana:
    image: docker.elastic.co/kibana/kibana:9.0.4
    container_name: kibana
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    depends_on:
      - elasticsearch

    profiles:
      - dev

  suricata:
    image: jasonish/suricata:latest
    depends_on: [logstash]
    volumes:
      - pcapstore:/shared
      - ./suricata/suricata.yaml:/etc/suricata/suricata.yaml:ro
      - ./suricata/logs:/var/log/suricata
    command: >
      -c /etc/suricata/suricata.yaml
      --pcap-file-continuous
      -r /shared/suri
    profiles:
      - dev

  cicflowmeter:
    build: ./CICFlowMeter
    depends_on: [rabbitmq]
    environment:
      - PCAPS_DIR=/shared/cic
      - FLOWS_DIR=/flows
      - DELAY=5
      - CONVERT_LINKTYPE=1
    volumes:
      - pcapstore:/shared
      - ./CICFlowMeter/flows:/flows

  pcap-feeder:
    build: ./pcap-feeder
    depends_on:
      - suricata
      - cicflowmeter
    # host networking only if you want live capture; for offline CICIDS2018 replay set CAPTURE=0
    network_mode: host
    cap_add:
      - NET_ADMIN
      - NET_RAW
    environment:
      - CAP_IF=eth0          # set your real interface for live capture
      - CAPTURE=1            # 0 = disable capture (still fans-in from /shared/incoming)
    volumes:
      - pcapstore:/shared    # single volume so hard-links work


volumes:
  pcapstore:
  esdata_new_comparative:
