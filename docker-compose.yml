services:
  rabbitmq:
    image: rabbitmq:3-management
    ports:
      - "5672:5672"
      - "15672:15672"
    environment:
      RABBITMQ_DEFAULT_USER: ${RABBITMQ_DEFAULT_USER:-guest}
      RABBITMQ_DEFAULT_PASS: ${RABBITMQ_DEFAULT_PASS:-guest}
    profiles:
      - dev

  oraculo:
    build: ./Oraculo
    environment:
      RABBITMQ_DEFAULT_USER: ${RABBITMQ_DEFAULT_USER:-guest}
      RABBITMQ_DEFAULT_PASS: ${RABBITMQ_DEFAULT_PASS:-guest}
      LOGSTASH_URL: ${LOGSTASH_URL:-http://logstash:5044}
    depends_on:
      cicflowmeter:
        condition: service_healthy
      elasticsearch:
        condition: service_healthy
      logstash:
        condition: service_healthy
    ports:
      - "8000:8000"
    profiles:
      - dev
    # mem_limit: 8g

  mlflow:
    build: ./mlflow
    container_name: mlflow
    ports:
      - "5000:5000"
    environment:
      MLFLOW_BACKEND_URI: sqlite:////mlflow/mlflow.db
      MLFLOW_ARTIFACT_ROOT: /mlflow/artifacts
    volumes:
      - mlflow_data:/mlflow
    command: >
      mlflow server
      --host 0.0.0.0
      --port 5000
      --backend-store-uri ${MLFLOW_BACKEND_URI}
      --artifacts-destination ${MLFLOW_ARTIFACT_ROOT}

  cicflowmeter:
    build: ./CICFlowMeter
    depends_on: [rabbitmq]
    environment:
      PCAPS_DIR: ${PCAPS_DIR:-/shared/cic}
      FLOWS_DIR: ${FLOWS_DIR:-/flows}
      DELAY: ${DELAY:-5}
      CONVERT_LINKTYPE: ${CONVERT_LINKTYPE:-1}
    volumes:
      - ./pcapstore/cic:/pcaps
      - ./CICFlowMeter/flows:/flows
    healthcheck:
      test: ["CMD", "bash", "-c", "test -d ${FLOWS_DIR:-/flows}"]
      interval: 20s
      timeout: 3s
      retries: 15
    profiles:
      - dev

  pcap-feeder:
    build: ./pcap-feeder
    depends_on:
      - cicflowmeter
    # host networking only if you want live capture; for offline CICIDS2018 replay set CAPTURE=0
    network_mode: host
    cap_add:
      - NET_ADMIN
      - NET_RAW
    environment:
      CAP_IF: ${CAP_IF:-eth0} # set your real interface for live capture
      CAPTURE: ${CAPTURE:-1}  # 0 = disable capture (still fans-in from /shared/incoming)
      MIRROR_SURICATA: ${MIRROR_SURICATA:-false}
      MIRROR_ORACULO: ${MIRROR_ORACULO:-true}
    volumes:
      - ./pcapstore:/shared    # single volume so hard-links work

  # ---------- DEV ONLY CONTAINERS ----------

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:9.0.4
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - xpack.license.self_generated.type=basic
      - ES_JAVA_OPTS=-Xms${ELASTICSEARCH_MEMORY_LIMIT:-256m} -Xmx${ELASTICSEARCH_MEMORY_LIMIT:-256m}
    ports:
      - "9200:9200"
    volumes:
      - esdata_new_comparative:/usr/share/elasticsearch/data
    profiles:
      - dev
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:9200"]
      interval: 10s
      timeout: 3s
      retries: 30

  logstash:
    image: docker.elastic.co/logstash/logstash:9.0.4
    container_name: logstash
    depends_on:
      - elasticsearch
    volumes:
      - ${LOGSTASH_CONFIG_PATH:-./logstash/pipeline}:/usr/share/logstash/pipeline:rw
    ports:
      - "5044:5044"
      - "9600:9600"
    environment:
      - LS_JAVA_OPTS=-Xms${LOGSTASH_MEMORY_LIMIT:-512m} -Xmx${LOGSTASH_MEMORY_LIMIT:-512m}
    profiles:
      - dev
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:9600/_node/pipelines"]
      interval: 10s
      timeout: 3s
      retries: 30
    

  kibana:
    image: docker.elastic.co/kibana/kibana:9.0.4
    container_name: kibana
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=${ES_URL:-http://elasticsearch:9200}
    depends_on:
      - elasticsearch
    profiles:
      - dev

  suricata:
    build: ./suricata
    container_name: project_mpc-suricata-1
    depends_on: [logstash]
    volumes:
      - ./pcaps:/shared/suri
      - ${SURICATA_CONFIG_PATH:-./suricata/suricata.yaml}:/etc/suricata/suricata.yaml:ro
      - ./suricata/suricata-metrics.yaml:/suricata-metrics.yaml:ro
      - ./suricata/logs:/var/log/suricata
      - ./suricata/rules:/etc/suricata/rules
    ports:
      - "9101:9101"
    healthcheck:
      test: ["CMD-SHELL", "test -s /var/log/suricata/eve.json"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    profiles:
      - dev

  flow-aggregator:
    image: python:3.11-slim
    container_name: project_mpc-flow-aggregator
    depends_on:
      - suricata
    volumes:
      - ./suricata/logs:/var/log/suricata
      - ./suricata/flow_aggregator.py:/flow_aggregator.py
    working_dir: /var/log/suricata
    command: ["python3", "/flow_aggregator.py"]
    profiles:
      - dev


  # grafana:
  #   image: grafana/grafana:latest
  #   profiles: ["dev"]
  #   env_file:
  #     - .env.${COMPOSE_PROFILE:-dev}
  #   environment:
  #     GF_SECURITY_ADMIN_USER: ${GRAFANA_ADMIN_USER:-admin}
  #     GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD:-admin}
  #   ports:
  #     - "3000:3000"
  #   volumes:
  #     - grafana_data:/var/lib/grafana
  #   depends_on:
  #     - elasticsearch
  #   # Optional: auto-provision ES datasource by mounting ./grafana/provisioning

  prometheus:
    image: prom/prometheus:v2.53.0
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prom_data:/prometheus
    ports: ["9090:9090"]
    profiles:
      - dev

  pushgateway:
    image: prom/pushgateway:v1.9.0
    ports: ["9091:9091"]
    profiles:
      - dev

  telegraf:
    image: telegraf:1.31
    container_name: telegraf
    volumes:
      - ./suricata/logs:/var/log/suricata:ro
      - ./telegraf/telegraf.conf:/etc/telegraf/telegraf.conf:ro
      - ./telegraf/scripts:/etc/telegraf/scripts:ro
    ports:
      - "9273:9273"
    profiles:
      - dev
    depends_on:
      - suricata


volumes:
  pcapstore:
  esdata_new_comparative:
  mlflow_data:
  comparative_data:
  prom_data:
  grafana_data: